{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end evaluation\n",
    "\n",
    "<!-- ## Experiment Overview\n",
    "\n",
    "| **Topic**                 | Description                                                                                                                                                                                                                                                                                                         |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| 📝 **Hypothesis**         | Exploratory hypothesis: \"Can introducing a new language model improve the system's performance?\"                                                                                                                                                                                                                    |\n",
    "| ⚖️ **Comparison**         | We will compare **GPT3-3.5** (from OpenAI) to **Mistral**(open-source)                                                                                                                                                                                                                                              |\n",
    "| 🎯 **Evaluation Metrics** | We will look at human-centric metrics ([Groundedness, Relevance, Coherence, Similarity, Fluency](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2)) using another LLM as judge approach to compare the performance |\n",
    "| 📊 **Evaluation Dataset** | 300 question-answer pairs generated from [code-with-engineering](../data/docs/code-with-engineering/) and [code-with-mlops](../data/docs/code-with-mlops/) sections from Solution Ops repository.                                                                                                                   | -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this section we will evaluate end-to-end our RAG system using human-centric metrics ([Groundedness, Relevance, Coherence, Similarity, Fluency](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2)). We will use those metrics to evaluate the [baseline](./2.rag-implementation.ipynb) approach we started with to the new RAG application which takes into account the conclusions from our previous experiments.\n",
    " \n",
    "\n",
    "<!-- as well as [ROUGE](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499), which is a traditional evaluation metric. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Create a new index -->\n",
    "After performing the previous two experiments, we concluded the following:\n",
    "- Based on [experiment 3.1](./3.1.experiment_embedding.ipynb), the embedding model with the best score was intfloat/e5-small-v2.\n",
    "- Based on [experiment 3.2](./3.2.experiment_chunking.ipynb), the chunking strategy that returned the best results was the semantic chunking.\n",
    "<!-- \n",
    "Let's create a new index, where we upload the data which was chunked using the semantic chunking and embedded using infloat/e5-small-v2. In the interest of time, we have prepared the data and you can find it at [semantic-chunking-engineering-mlops-e5-small-v2.json](./output/pre-generated/embeddings/semantic-chunking-engineering-mlops-e5-small-v2.json). The path to the file is configured in `pregenerated_semantic_chunks_embeddings_os` variable. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answers \n",
    "\n",
    "In order to evaluate our two systems, we will generate answers for each question in our [evaluation dataset](./output/qa//evaluation/qa_pairs_solutionops.json).\n",
    "For each question:\n",
    "- We embed the question.\n",
    "- We search for relevant documents in the search index.\n",
    "- We sent to the LLM the question as well as the documents.\n",
    "- We get an answer back.\n",
    "\n",
    "How *good* that answer is will be judged from a groundedness, relevance, coherence, similarity and fluency perspective using another LLM as judge.\n",
    "\n",
    "```{note}\n",
    "In the interest of time, we have run the above steps using [this helper notebook](./helpers/qa.ipynb). The answers were saved as follows:\n",
    "- [fixed-size-chunks-180-30-engineering-mlops-ada](./output/qa/results/fixed-size-chunks-180-30-engineering-mlops-ada.json) holds the answers for our baseline RAG system where we used a fixed-size chunking strategy and AOI embedding model.\n",
    "- [semantic-chunking-intfloat.json](./output/qa/results/semantic-chunking-intfloat.json) holds the answers for the RAG system where we used semantic chunking and intfloat/e5-small-v2 embedding model.\n",
    "```\n",
    "<!-- The data is structured as such:\n",
    "- user_prompt: the question\n",
    "-output_prompt\": ,\n",
    "\"context\": ,\n",
    "\"chunk_id\": \"chunk47_10\",\n",
    "\"source\": \"..\\\\data\\\\docs\\\\code-with-engineering\\\\design\\\\design-reviews\\\\trade-studies\\\\template.md\",\n",
    "\"root_chunk_id\": \"chunk47\",\n",
    "\"generated_output\": ,\n",
    "\"retrieved_context\": \"\",\n",
    "\"retrieved_source\": \"..\\\\data\\\\docs\\\\code-with-engineering\\\\design\\\\design-reviews\\\\trade-studies\\\\README.md\",\n",
    "\"retrieved_chunk_id\": \"chunk403_2\"  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chat'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "import openai\n",
    "import mlflow\n",
    "import os\n",
    "azure_openai_key = os.environ[\"azure_openai_key\"]\n",
    "azure_aoai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "aoi_deployment_name = os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"]\n",
    "\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", azure_openai_key)\n",
    "os.environ.setdefault(\"OPENAI_API_BASE\", azure_aoai_endpoint)\n",
    "os.environ.setdefault(\"OPENAI_API_VERSION\", \"2023-05-15\")\n",
    "os.environ.setdefault(\"OPENAI_API_TYPE\", \"azure\")\n",
    "os.environ.setdefault(\"OPENAI_DEPLOYMENT_NAME\", aoi_deployment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human-Centric Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate these metrics, we will take the [LLM as Judge approach](https://arxiv.org/pdf/2311.09476.pdf) and will use another LLM as the judge. \n",
    "\n",
    "We will use MLflow open-source tool, more precisely, the [MLflow LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#mlflow-llm-evaluate) API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LLM as Judge](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#metrics-with-llm-as-the-judge)\n",
    "Set the LLM model that you would like to use as judge.\n",
    "Note: This steps assumes that you have previously deployed this model in your Azure OpenAI resource group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model = \"openai:/gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%run -i ./pre-requisites.ipynb\n",
    "%run -i ./helpers/search.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness metric\n",
    "\n",
    "This metric evaluates how faithful the model's generated output is to the context provided. High scores mean that the outputs contain information that is in line with the context, while low scores mean that outputs may disagree with the context (input is ignored). It uses [Likert scale](https://en.wikipedia.org/wiki/Likert_scale).\n",
    "\n",
    "This metric is similar to [Groundedness](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2#groundedness) from Azure Machine Learning or Prompt Flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import faithfulness, EvaluationExample\n",
    "\n",
    "\n",
    "# Create a good and bad example for faithfulness in the context of this problem\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \",\n",
    "        score=2,\n",
    "        justification=\"The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions.\",\n",
    "        score=5,\n",
    "        justification=\"The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(\n",
    "    model=judge_model, examples=faithfulness_examples)\n",
    "\n",
    "# print(faithfulness_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance metric\n",
    "\n",
    "This metric evaluates how relevant the model's generated output is with respect to both the input and the provided context. High scores mean that the model has understood the context and correct extracted relevant information from the context, while low score mean that output has completely ignored the question and the context and could be hallucinating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import relevance\n",
    "\n",
    "\n",
    "relevance_metric = relevance(model=judge_model)\n",
    "# print(relevance_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity metric\n",
    "\n",
    "This metric evaluates how similar the model's generated output is compared to the information in the ground_truth. High scores mean that your model outputs contain similar information as the ground_truth, while low scores mean that outputs may disagree with the ground_truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import answer_similarity\n",
    "similarity_metric = answer_similarity(model=judge_model)\n",
    "# print(similarity_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluency metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no pre-defined fluency metric in MLflow. However, we can create [custom LLM metrics](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#creating-custom-llm-evaluation-metrics).\n",
    "\n",
    "We will define fluency in the exact same way as it is defined in [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/ai/azure-ai-generative/azure/ai/generative/evaluate/pf_templates/built_in_metrics/qa/gpt_fluency_prompt.jinja2).\n",
    "\n",
    "Therefore, fluency will measure the quality of individual sentences in the answer, and whether they are well-written and grammatically correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency_example_1 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What did you have for breakfast today?\",\n",
    "    output=\"Breakfast today, me eating cereal and orange juice very good.\",\n",
    "    justification=\"The answer completely lacks fluency\",\n",
    "    score=1,\n",
    ")\n",
    "fluency_example_2 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"How do you feel when you travel alone?\",\n",
    "    output=\"Alone travel, nervous, but excited also. I feel adventure and like its time.\",\n",
    "    justification=\"The answer mostly lacks fluency\",\n",
    "    score=2,\n",
    ")\n",
    "fluency_example_3 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"When was the last time you went on a family vacation?\",\n",
    "    output=\"Alone travel, nervous, but excited also. I feel adventure and like its time.\",\n",
    "    justification=\"The answer is partially fluent\",\n",
    "    score=3,\n",
    ")\n",
    "fluency_example_4 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is your favorite thing about your job?\",\n",
    "    output=\"My favorite aspect of my job is the chance to interact with diverse people. I am constantly learning from their experiences and stories.\",\n",
    "    justification=\"The answer is mostly fluent\",\n",
    "    score=4,\n",
    ")\n",
    "fluency_example_5 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"Can you describe your morning routine?\",\n",
    "    output=(\n",
    "        \"Every morning, I wake up at 6 am, drink a glass of water, and do some light stretching.\"\n",
    "        \"After that, I take a shower and get dressed for work. Then, I have a healthy breakfast, \"\n",
    "        \"usually consisting of oatmeal and fruits, before leaving the house around 7:30 am.\",\n",
    "    ),\n",
    "    justification=\"The answer is completely fluent\",\n",
    "    score=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"fluency\",\n",
    "    definition=(\n",
    "        \"Fluency measures the quality of individual sentences in the answer, and whether they are well-written and grammatically correct. \"\n",
    "        \"Consider the quality of individual sentences when evaluating fluency. \"\n",
    "        \"Given the question and answer, score the fluency of the answer between one to five stars using the following rating scale:\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"One star: the answer completely lacks fluency\"\n",
    "        \"Two stars: the answer mostly lacks fluency\"\n",
    "        \"Three stars: the answer is partially fluent\"\n",
    "        \"Four stars: the answer is mostly fluent\"\n",
    "        \"Five stars: the answer has perfect fluency\"\n",
    "    ),\n",
    "    examples=[fluency_example_1, fluency_example_2,\n",
    "              fluency_example_3, fluency_example_4, fluency_example_5],\n",
    "    model=judge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence\n",
    "\n",
    "There is no pre-defined coherence metric in MLflow. However, we can create [custom LLM metrics](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#creating-custom-llm-evaluation-metrics).\n",
    "\n",
    "We will define fluency in the exact same way as it is defined in [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/ai/azure-ai-generative/azure/ai/generative/evaluate/pf_templates/built_in_metrics/qa/gpt_coherence_prompt.jinja2).\n",
    "\n",
    "Therefore, coherence of an answer will be measured by how well all the sentences fit together and sound naturally as a whole.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_example_1 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is your favorite indoor activity and why do you enjoy it?\",\n",
    "    output=\"I like pizza. The sun is shining.\",\n",
    "    justification=\"The answer completely lacks fluency\",\n",
    "    score=1,\n",
    ")\n",
    "coherence_example_2 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"Can you describe your favorite movie without giving away any spoilers?\",\n",
    "    output=\"It is a science fiction movie. There are dinosaurs. The actors eat cake. People must stop the villain.\",\n",
    "    justification=\"The answer mostly lacks fluency\",\n",
    "    score=2,\n",
    ")\n",
    "coherence_example_3 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What are some benefits of regular exercise?\",\n",
    "    output=\"Regular exercise improves your mood. A good workout also helps you sleep better. Trees are green.\",\n",
    "    justification=\"The answer is partially fluent\",\n",
    "    score=3,\n",
    ")\n",
    "coherence_example_4 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"How do you cope with stress in your daily life?\",\n",
    "    output=\"I usually go for a walk to clear my head. Listening to music helps me relax as well. Stress is a part of life, but we can manage it through some activities.\",\n",
    "    justification=\"The answer is mostly fluent\",\n",
    "    score=4,\n",
    ")\n",
    "coherence_example_5 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What can you tell me about climate change and its effects on the environment?\",\n",
    "    output=(\n",
    "        \"Climate change has far-reaching effects on the environment. \"\n",
    "        \"Rising temperatures result in the melting of polar ice caps, contributing to sea-level rise. \"\n",
    "        \"Additionally, more frequent and severe weather events, such as hurricanes and heatwaves, can cause disruption to ecosystems and human societies alike.\"\n",
    "    ),\n",
    "    justification=\"The answer is completely fluent\",\n",
    "    score=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"coherence\",\n",
    "    definition=(\n",
    "        \"Coherence of an answer is measured by how well all the sentences fit together and sound naturally as a whole. \"\n",
    "        \"Consider the overall quality of the answer when evaluating coherence. \"\n",
    "        \"Given the question and answer, score the coherence of answer between one to five stars using the following rating scale:\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"One star: the answer completely lacks coherence\"\n",
    "        \"Two stars: the answer mostly lacks coherence\"\n",
    "        \"Three stars: the answer is partially coherent\"\n",
    "        \"Four stars: the answer is mostly coherent\"\n",
    "        \"Five stars: the answer has perfect coherency\"\n",
    "    ),\n",
    "    examples=[coherence_example_1, coherence_example_2,\n",
    "              coherence_example_3, coherence_example_4, coherence_example_5],\n",
    "    model=judge_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a job (experiment) in Azure Machine Learning Studio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "subscription_id = os.environ[\"subscription_id\"]\n",
    "resource_group_name = os.environ[\"resource_group_name\"]\n",
    "workspace_name = os.environ[\"workspace_name\"]\n",
    "\n",
    "# experiment_name = \"semantic-chunking-intfloat-e5-small-v2\"\n",
    "# mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# !az login\n",
    "ws = Workspace.get(name=workspace_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group_name)\n",
    "\n",
    "mlflow_tracking_uri = ws.get_mlflow_tracking_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate(input_path, experiment_name, run_name):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        qa_evluation_data = json.load(file)\n",
    "        df = pd.DataFrame.from_records(qa_evluation_data)\n",
    "        df[\"user_prompt\"] = df[\"user_prompt\"].astype(str)\n",
    "        df[\"output_prompt\"] = df[\"output_prompt\"].astype(str)\n",
    "        df[\"retrieved_context\"] = df[\"retrieved_context\"].astype(str)\n",
    "        df[\"generated_output\"] = df[\"generated_output\"].astype(str)\n",
    "\n",
    "        mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "            results = mlflow.evaluate(data=df,\n",
    "                                      # predictions=\"output_prompt\",\n",
    "                                      predictions=\"generated_output\",\n",
    "                                      model_type=\"question-answering\",\n",
    "                                      extra_metrics=[\n",
    "                                          faithfulness_metric,\n",
    "                                          relevance_metric,\n",
    "                                          coherence,\n",
    "                                          fluency,\n",
    "                                          #   similarity_metric,\n",
    "                                        #   mlflow.metrics.latency()\n",
    "                                          ],\n",
    "                                      evaluator_config={\n",
    "                                          \"col_mapping\": {\n",
    "                                              \"inputs\": \"user_prompt\",  # Define the column name for the input\n",
    "                                              # \"context\": \"context\",\n",
    "                                              \"context\": \"retrieved_context\",\n",
    "                                              #   \"targets\": \"output_prompt\"\n",
    "                                          }\n",
    "                                      })\n",
    "\n",
    "            # mlflow.log_metric('toxicity', results.metrics['toxicity/v1/p90'])\n",
    "            mlflow.log_metric('faithfulness_mean',\n",
    "                              results.metrics['faithfulness/v1/mean'])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluate the baseline RAG solution model \n",
    "<!-- fixed-size chunking strategy with ada  -->\n",
    "Took > 12 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "experiment_name = \"fixed-size-180-30-ada\" #TODO: Change this to the name of the experiment\n",
    "mlflow.create_experiment(experiment_name)\n",
    "\n",
    "input_path = \"./output/qa/results/fixed-size-chunks-180-30-engineering-mlops-ada.json\"\n",
    "run_name = \"4metrics\"\n",
    "results_fixed_size_ada = evaluate(input_path, experiment_name, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate the new RAG solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "experiment_name = \"semantic-chunking-intfloat-e5-small-v2\" #TODO: Change this to the name of the experiment\n",
    "mlflow.create_experiment(experiment_name)\n",
    "\n",
    "input_path = \"./output/qa/results/semantic-chunking-intfloat.json\"\n",
    "run_name = \"4metrics\" #TODO: Change this to the name of the run\n",
    "results_semantic_chunking = evaluate(input_path, experiment_name, run_name)\n",
    "\n",
    "print(results_semantic_chunking.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Relevance  Faithfulness  Coherence   Fluency\n",
      "semantic_chunking_eval    3.470000      3.806020   3.530201  3.508418\n",
      "fixed_size_chunking_ada   2.276667      2.675585   2.590000  2.622896\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results_semantic_chunking = {'latency/mean': 0.0,\n",
    "                             'latency/variance': 0.0,\n",
    "                             'latency/p90': 0.0,\n",
    "                             'toxicity/v1/mean': 0.0005107220515492372,\n",
    "                             'toxicity/v1/variance': 2.3427437314922485e-06,\n",
    "                             'toxicity/v1/p90': 0.0006927762471605093,\n",
    "                             'toxicity/v1/ratio': 0.0,\n",
    "                             'faithfulness/v1/mean': 3.8060200668896322,\n",
    "                             'faithfulness/v1/variance': 1.4406326551157145,\n",
    "                             'faithfulness/v1/p90': 5.0,\n",
    "                             'relevance/v1/mean': 3.47,\n",
    "                             'relevance/v1/variance': 1.3824333333333334,\n",
    "                             'relevance/v1/p90': 5.0,\n",
    "                             'coherence/v1/mean': 3.530201342281879,\n",
    "                             'coherence/v1/variance': 0.7323093554344399,\n",
    "                             'coherence/v1/p90': 4.0,\n",
    "                             'fluency/v1/mean': 3.508417508417508,\n",
    "                             'fluency/v1/variance': 0.6876395832624789,\n",
    "                             'fluency/v1/p90': 4.0}\n",
    "\n",
    "results_fixed_size_ada = {'latency/mean': 0.0,\n",
    "                          'latency/variance': 0.0,\n",
    "                          'latency/p90': 0.0,\n",
    "                          'toxicity/v1/mean': 0.0002763730055691364,\n",
    "                          'toxicity/v1/variance': 1.1839384498113026e-07,\n",
    "                          'toxicity/v1/p90': 0.0005062455020379276,\n",
    "                          'toxicity/v1/ratio': 0.0,\n",
    "                          'faithfulness/v1/mean': 2.6755852842809364,\n",
    "                          'faithfulness/v1/variance': 2.132213286204852,\n",
    "                          'faithfulness/v1/p90': 5.0,\n",
    "                          'relevance/v1/mean': 2.276666666666667,\n",
    "                          'relevance/v1/variance': 1.7534555555555555,\n",
    "                          'relevance/v1/p90': 4.0,\n",
    "                          'coherence/v1/mean': 2.59,\n",
    "                          'coherence/v1/variance': 1.0485666666666669,\n",
    "                          'coherence/v1/p90': 4.0,\n",
    "                          'fluency/v1/mean': 2.622895622895623,\n",
    "                          'fluency/v1/variance': 0.9487013796778109,\n",
    "                          'fluency/v1/p90': 4.0}\n",
    "\n",
    "\n",
    "keys = ['relevance/v1/mean', 'faithfulness/v1/mean',\n",
    "\n",
    "        'coherence/v1/mean', 'fluency/v1/mean']\n",
    "\n",
    "semantic_chunking_eval_values = [\n",
    "\n",
    "    results_semantic_chunking[key] for key in keys]\n",
    "\n",
    "fixed_size_chunking_ada_values = [\n",
    "\n",
    "    results_fixed_size_ada[key] for key in keys]\n",
    "\n",
    "table = [semantic_chunking_eval_values, fixed_size_chunking_ada_values]\n",
    "\n",
    "df = pd.DataFrame(table, columns=['Relevance', 'Faithfulness', 'Coherence', 'Fluency'], index=[\n",
    "\n",
    "                  'semantic_chunking_eval', 'fixed_size_chunking_ada'])\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
